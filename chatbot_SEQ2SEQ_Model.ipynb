{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "CIyIiUiz2P8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59ABIwAYAojX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import re \n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "Np_EGprxc1fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [sentence.replace(\"'\", \"\") for sentence in question]\n",
        "answers = [sentence.replace(\"'\", \"\") for sentence in answer]\n"
      ],
      "metadata": {
        "id": "BMbbzwq9zvVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversations[:5]\n",
        "questions[:5]"
      ],
      "metadata": {
        "id": "XuF_9BffCUNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a5d40a-32dc-4c47-aaa8-686430adab33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[What does this section show different approaches to working with chained maps?',\n",
              " ' What is an example of simulating a Pythonâ€™s internal lookup chain?',\n",
              " ' What do environment variables take precedence over?',\n",
              " ' What class can be used to simulate nested contexts?',\n",
              " ' What does the ChainMap class only make updates to?']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lines[:5]\n",
        "answers[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6EQCRX8pwTM",
        "outputId": "75a2454d-fa8c-49f1-edca-5a179e424976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[various approaches to working with chained maps.',\n",
              " ' Example',\n",
              " ' default values',\n",
              " ' ChainMap',\n",
              " ' the  first mapping in the chain']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#len(id2line.keys())"
      ],
      "metadata": {
        "id": "3JVQMiyJyOSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversations_id[:10]"
      ],
      "metadata": {
        "id": "WQ_0BEpz1Dog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21EDzsTc2fzx",
        "outputId": "9830e1be-f402-4102-8a77-75e62adc49c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbt2T6Jc2f2Y",
        "outputId": "803fece1-970f-48c1-8810-6a957bc03fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqx4WCxV6civ",
        "outputId": "e5a71b63-a432-4daf-db87-11c06c7a11df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers = answers[:152]"
      ],
      "metadata": {
        "id": "f3ZT-b_A6Wkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing a first cleaning of the texts\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ptgv9QU92f45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_clean = []\n",
        "for question in questions:\n",
        "    questions_clean.append(clean_text(question))\n",
        "\n",
        "answers_clean = []\n",
        "for answer in answers:\n",
        "    answers_clean.append(clean_text(answer))"
      ],
      "metadata": {
        "id": "8nqTp5h22f70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in questions_clean:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in answers_clean:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "metadata": {
        "id": "JNOBNFCj2f-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word2count"
      ],
      "metadata": {
        "id": "4tCw84vCvJYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 1\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 1\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1"
      ],
      "metadata": {
        "id": "y2r4xCsb1GlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#answerswords2int"
      ],
      "metadata": {
        "id": "4Py6Vwvzu1Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the last tokens to these two dictionaries, to use it in encoding and decoding.\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1"
      ],
      "metadata": {
        "id": "EXWmBh6Bglpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the inverse dictionary of the answerswords2int dictionary, to use it in seq2seq model\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
      ],
      "metadata": {
        "id": "z3K6ZElTglsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(answers_clean)):\n",
        "    answers_clean[i] += ' <EOS>'"
      ],
      "metadata": {
        "id": "xYpyn7k9glvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_clean[:3]"
      ],
      "metadata": {
        "id": "--6tKHueglxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80627bc1-b132-41d7-b13c-690eba42502d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[various approaches to working with chained maps <EOS>',\n",
              " ' example <EOS>',\n",
              " ' default values <EOS>']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in questions_clean:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "\n",
        "\n",
        "answers_into_int = []\n",
        "for answer in answers_clean:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)"
      ],
      "metadata": {
        "id": "nA6RvrcYgl0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "metadata": {
        "id": "Fz2DCmGOgl27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "33mlLi8egl9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smf3yo0vgl_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6Dkpf2SgmBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-wGd2cG2gA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y3-cSNAi2gDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING THE SEQ2SEQ MODEL"
      ],
      "metadata": {
        "id": "hVR1bfPB11fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "DdPBx9HB3jjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow_addons.seq2seq import BahdanauAttention, Decoder, Sampler\n",
        "\n",
        "\n",
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = Input(shape=(None, ), dtype=tf.int32, name='input')\n",
        "    targets = Input(shape=(None, ), dtype=tf.int32, name='target')\n",
        "    lr = Input(shape=(), dtype=tf.float32, name='learning_rate')\n",
        "    keep_prob = Input(shape=(), dtype=tf.float32, name='keep_prob')\n",
        "    return inputs, targets, lr, keep_prob\n",
        "\n",
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = targets[:, :-1]\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], axis=1)\n",
        "    return preprocessed_targets\n",
        "\n",
        "# Creating the Encoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, num_layers, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_layers = [LSTM(rnn_units, return_sequences=True, return_state=True, dropout=dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        for layer in self.rnn_layers:\n",
        "            x, *states = layer(x)\n",
        "        return x, states\n",
        "\n",
        "# Creating the Decoder\n",
        "class CustomDecoder(Decoder):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, num_layers, dropout_rate):\n",
        "        super(CustomDecoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_layers = [LSTM(rnn_units, return_sequences=True, return_state=True, dropout=dropout_rate) for _ in range(num_layers)]\n",
        "        self.attention = BahdanauAttention(rnn_units)\n",
        "        self.dense = Dense(vocab_size, activation=\"linear\")\n",
        "\n",
        "    def initialize(self, initial_state, **kwargs):\n",
        "        self.initial_state = initial_state\n",
        "        return initial_state\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None, **kwargs):\n",
        "        x = self.embedding(inputs)\n",
        "        outputs = []\n",
        "        for layer in self.rnn_layers:\n",
        "            x, *states = layer(x, initial_state=self.initial_state)\n",
        "            self.initial_state = states\n",
        "            context_vector, _ = self.attention(x, **kwargs)\n",
        "            x = tf.concat([x, context_vector], axis=-1)\n",
        "            outputs.append(x)\n",
        "        x = self.dense(tf.concat(outputs, axis=1))\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_seq2seq_model(batch_size, answers_vocab_size, questions_vocab_size, encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(batch_shape=(batch_size, None), name='encoder_inputs')\n",
        "    encoder_embeddings = Embedding(questions_vocab_size + 1, encoding_embedding_size, mask_zero=True, name='encoder_embeddings')(encoder_inputs)\n",
        "    encoder_lstm = LSTM(rnn_size, return_sequences=True, return_state=True, name='encoder_lstm')\n",
        "    _, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(batch_shape=(batch_size, None), name='decoder_inputs')\n",
        "    decoder_embeddings = Embedding(answers_vocab_size + 1, decoding_embedding_size, mask_zero=True, name='decoder_embeddings')(decoder_inputs)\n",
        "    decoder_lstm = LSTM(rnn_size, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=[encoder_state_h, encoder_state_c])\n",
        "\n",
        "    # Output layer\n",
        "    decoder_dense = Dense(answers_vocab_size + 1, activation='softmax', name='decoder_dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Create the seq2seq model\n",
        "    seq2seq_model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return seq2seq_model\n"
      ],
      "metadata": {
        "id": "gUU2-Ht41-2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6_8d1j71_YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING THE SEQ2SEQ MODEL"
      ],
      "metadata": {
        "id": "Wy7Cg9uo2s9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "# Setting the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5\n",
        "\n",
        "# Loading the model inputs\n",
        "inputs, targets, lr, keep_prob = model_inputs()\n",
        "\n",
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)\n",
        "\n",
        "# Create the seq2seq model\n",
        "seq2seq_model = create_seq2seq_model(batch_size, len(answerswords2int), len(questionswords2int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, questionswords2int)\n",
        "# Compile the model\n",
        "seq2seq_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "\n",
        "\n",
        "# Padding the sequences with the <PAD> token\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
        "\n",
        "# Splitting the data into batches of questions and answers\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "        yield padded_questions_in_batch, padded_answers_in_batch\n",
        "\n",
        "# Splitting the questions and answers into training and validation sets\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "training_questions = sorted_clean_questions[training_validation_split:]\n",
        "training_answers = sorted_clean_answers[training_validation_split:]\n",
        "validation_questions = sorted_clean_questions[:training_validation_split]\n",
        "validation_answers = sorted_clean_answers[:training_validation_split]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class LossCalculationLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(LossCalculationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        logits, targets, sequence_length = inputs\n",
        "        loss = tf.keras.backend.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
        "        mask = tf.sequence_mask(sequence_length, dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "# Instantiate the loss calculation layer\n",
        "loss_calculation_layer = LossCalculationLayer()\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 2000\n",
        "\n",
        "# Training\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = seq2seq_model([padded_questions_in_batch, padded_answers_in_batch[:, :-1]])\n",
        "            loss_error = loss_calculation_layer([logits, padded_answers_in_batch[:, 1:], padded_answers_in_batch.shape[1] - 1])\n",
        "        #total_training_loss_error += loss_error.numpy()\n",
        "        gradients = tape.gradient(loss_error, seq2seq_model.trainable_variables)\n",
        "        clipped_gradients = [tf.clip_by_value(grad, -5., 5.) for grad in gradients]\n",
        "        optimizer.apply_gradients(zip(clipped_gradients, seq2seq_model.trainable_variables))\n",
        "        # ...\n",
        "        history = seq2seq_model.fit([padded_questions_in_batch, padded_answers_in_batch], padded_answers_in_batch, batch_size=batch_size, epochs=1, verbose=0)\n",
        "        training_loss = history.history['loss'][0]\n",
        "        print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}'.format(epoch, epochs, batch_index, len(training_questions) // batch_size, training_loss))\n",
        "\n",
        "    # Validation\n",
        "    total_validation_loss = 0\n",
        "    total_batches = 0\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "        validation_loss = seq2seq_model.evaluate([padded_questions_in_batch, padded_answers_in_batch], padded_answers_in_batch, batch_size=batch_size, verbose=0)\n",
        "        total_validation_loss += validation_loss\n",
        "        total_batches += 1\n",
        "\n",
        "    #average_validation_loss = total_validation_loss / total_batches\n",
        "    if total_batches > 0:\n",
        "        average_validation_loss = total_validation_loss / total_batches\n",
        "    else:\n",
        "        average_validation_loss = None\n",
        "    #print('Validation Loss Error: {:>6.3f}'.format(average_validation_loss))\n",
        "    if average_validation_loss is not None:\n",
        "        print('Validation Loss Error: {:>6.3f}'.format(average_validation_loss))\n",
        "    else:\n",
        "        print('No validation batches were processed.')\n",
        "\n",
        "    learning_rate *= learning_rate_decay\n",
        "    if learning_rate < min_learning_rate:\n",
        "        learning_rate = min_learning_rate\n",
        "        K.set_value(seq2seq_model.optimizer.learning_rate, learning_rate)\n",
        "\n",
        "    if average_validation_loss is not None and average_validation_loss <= best_validation_loss:\n",
        "        print('I speak better now!!')\n",
        "        early_stopping_check = 0\n",
        "        best_validation_loss = average_validation_loss\n",
        "        seq2seq_model.save_weights(checkpoint)\n",
        "    else:\n",
        "        print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "        early_stopping_check += 1\n",
        "        if early_stopping_check == early_stopping_stop:\n",
        "            break\n",
        "\n",
        "if early_stopping_check == early_stopping_stop:\n",
        "    print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "else:\n",
        "    print(\"Game Over\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bVmuRV41_au",
        "outputId": "831f9b92-20fd-4b38-93e3-192a69ded445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1/100, Batch:    0/4, Training Loss Error:  5.092\n",
            "Epoch:   1/100, Batch:    1/4, Training Loss Error:  2.488\n",
            "Epoch:   1/100, Batch:    2/4, Training Loss Error:  2.142\n",
            "Epoch:   1/100, Batch:    3/4, Training Loss Error:  2.742\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   2/100, Batch:    0/4, Training Loss Error:  0.930\n",
            "Epoch:   2/100, Batch:    1/4, Training Loss Error:  2.005\n",
            "Epoch:   2/100, Batch:    2/4, Training Loss Error:  1.451\n",
            "Epoch:   2/100, Batch:    3/4, Training Loss Error:  1.989\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   3/100, Batch:    0/4, Training Loss Error:  1.283\n",
            "Epoch:   3/100, Batch:    1/4, Training Loss Error:  1.748\n",
            "Epoch:   3/100, Batch:    2/4, Training Loss Error:  1.330\n",
            "Epoch:   3/100, Batch:    3/4, Training Loss Error:  1.989\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   4/100, Batch:    0/4, Training Loss Error:  0.721\n",
            "Epoch:   4/100, Batch:    1/4, Training Loss Error:  1.603\n",
            "Epoch:   4/100, Batch:    2/4, Training Loss Error:  1.154\n",
            "Epoch:   4/100, Batch:    3/4, Training Loss Error:  1.749\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   5/100, Batch:    0/4, Training Loss Error:  0.668\n",
            "Epoch:   5/100, Batch:    1/4, Training Loss Error:  1.524\n",
            "Epoch:   5/100, Batch:    2/4, Training Loss Error:  1.129\n",
            "Epoch:   5/100, Batch:    3/4, Training Loss Error:  1.662\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   6/100, Batch:    0/4, Training Loss Error:  0.669\n",
            "Epoch:   6/100, Batch:    1/4, Training Loss Error:  1.470\n",
            "Epoch:   6/100, Batch:    2/4, Training Loss Error:  1.098\n",
            "Epoch:   6/100, Batch:    3/4, Training Loss Error:  1.590\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   7/100, Batch:    0/4, Training Loss Error:  0.632\n",
            "Epoch:   7/100, Batch:    1/4, Training Loss Error:  1.404\n",
            "Epoch:   7/100, Batch:    2/4, Training Loss Error:  1.051\n",
            "Epoch:   7/100, Batch:    3/4, Training Loss Error:  1.549\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   8/100, Batch:    0/4, Training Loss Error:  0.606\n",
            "Epoch:   8/100, Batch:    1/4, Training Loss Error:  1.358\n",
            "Epoch:   8/100, Batch:    2/4, Training Loss Error:  1.022\n",
            "Epoch:   8/100, Batch:    3/4, Training Loss Error:  1.541\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   9/100, Batch:    0/4, Training Loss Error:  0.587\n",
            "Epoch:   9/100, Batch:    1/4, Training Loss Error:  1.318\n",
            "Epoch:   9/100, Batch:    2/4, Training Loss Error:  0.990\n",
            "Epoch:   9/100, Batch:    3/4, Training Loss Error:  1.499\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  10/100, Batch:    0/4, Training Loss Error:  0.565\n",
            "Epoch:  10/100, Batch:    1/4, Training Loss Error:  1.275\n",
            "Epoch:  10/100, Batch:    2/4, Training Loss Error:  0.959\n",
            "Epoch:  10/100, Batch:    3/4, Training Loss Error:  1.434\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  11/100, Batch:    0/4, Training Loss Error:  0.543\n",
            "Epoch:  11/100, Batch:    1/4, Training Loss Error:  1.237\n",
            "Epoch:  11/100, Batch:    2/4, Training Loss Error:  0.930\n",
            "Epoch:  11/100, Batch:    3/4, Training Loss Error:  1.372\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  12/100, Batch:    0/4, Training Loss Error:  0.523\n",
            "Epoch:  12/100, Batch:    1/4, Training Loss Error:  1.201\n",
            "Epoch:  12/100, Batch:    2/4, Training Loss Error:  0.902\n",
            "Epoch:  12/100, Batch:    3/4, Training Loss Error:  1.313\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  13/100, Batch:    0/4, Training Loss Error:  0.542\n",
            "Epoch:  13/100, Batch:    1/4, Training Loss Error:  1.177\n",
            "Epoch:  13/100, Batch:    2/4, Training Loss Error:  0.902\n",
            "Epoch:  13/100, Batch:    3/4, Training Loss Error:  1.260\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  14/100, Batch:    0/4, Training Loss Error:  0.511\n",
            "Epoch:  14/100, Batch:    1/4, Training Loss Error:  1.128\n",
            "Epoch:  14/100, Batch:    2/4, Training Loss Error:  0.881\n",
            "Epoch:  14/100, Batch:    3/4, Training Loss Error:  1.241\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  15/100, Batch:    0/4, Training Loss Error:  0.494\n",
            "Epoch:  15/100, Batch:    1/4, Training Loss Error:  1.081\n",
            "Epoch:  15/100, Batch:    2/4, Training Loss Error:  0.822\n",
            "Epoch:  15/100, Batch:    3/4, Training Loss Error:  1.121\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  16/100, Batch:    0/4, Training Loss Error:  0.473\n",
            "Epoch:  16/100, Batch:    1/4, Training Loss Error:  1.015\n",
            "Epoch:  16/100, Batch:    2/4, Training Loss Error:  0.758\n",
            "Epoch:  16/100, Batch:    3/4, Training Loss Error:  1.060\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  17/100, Batch:    0/4, Training Loss Error:  0.446\n",
            "Epoch:  17/100, Batch:    1/4, Training Loss Error:  0.943\n",
            "Epoch:  17/100, Batch:    2/4, Training Loss Error:  0.700\n",
            "Epoch:  17/100, Batch:    3/4, Training Loss Error:  0.963\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  18/100, Batch:    0/4, Training Loss Error:  0.417\n",
            "Epoch:  18/100, Batch:    1/4, Training Loss Error:  0.861\n",
            "Epoch:  18/100, Batch:    2/4, Training Loss Error:  0.641\n",
            "Epoch:  18/100, Batch:    3/4, Training Loss Error:  0.883\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  19/100, Batch:    0/4, Training Loss Error:  0.400\n",
            "Epoch:  19/100, Batch:    1/4, Training Loss Error:  0.816\n",
            "Epoch:  19/100, Batch:    2/4, Training Loss Error:  0.619\n",
            "Epoch:  19/100, Batch:    3/4, Training Loss Error:  0.836\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  20/100, Batch:    0/4, Training Loss Error:  0.382\n",
            "Epoch:  20/100, Batch:    1/4, Training Loss Error:  0.775\n",
            "Epoch:  20/100, Batch:    2/4, Training Loss Error:  0.594\n",
            "Epoch:  20/100, Batch:    3/4, Training Loss Error:  0.787\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  21/100, Batch:    0/4, Training Loss Error:  0.365\n",
            "Epoch:  21/100, Batch:    1/4, Training Loss Error:  0.700\n",
            "Epoch:  21/100, Batch:    2/4, Training Loss Error:  0.546\n",
            "Epoch:  21/100, Batch:    3/4, Training Loss Error:  0.742\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  22/100, Batch:    0/4, Training Loss Error:  0.343\n",
            "Epoch:  22/100, Batch:    1/4, Training Loss Error:  0.647\n",
            "Epoch:  22/100, Batch:    2/4, Training Loss Error:  0.534\n",
            "Epoch:  22/100, Batch:    3/4, Training Loss Error:  0.741\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  23/100, Batch:    0/4, Training Loss Error:  0.331\n",
            "Epoch:  23/100, Batch:    1/4, Training Loss Error:  0.752\n",
            "Epoch:  23/100, Batch:    2/4, Training Loss Error:  0.736\n",
            "Epoch:  23/100, Batch:    3/4, Training Loss Error:  1.349\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  24/100, Batch:    0/4, Training Loss Error:  0.559\n",
            "Epoch:  24/100, Batch:    1/4, Training Loss Error:  1.384\n",
            "Epoch:  24/100, Batch:    2/4, Training Loss Error:  1.123\n",
            "Epoch:  24/100, Batch:    3/4, Training Loss Error:  1.778\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  25/100, Batch:    0/4, Training Loss Error:  0.549\n",
            "Epoch:  25/100, Batch:    1/4, Training Loss Error:  1.279\n",
            "Epoch:  25/100, Batch:    2/4, Training Loss Error:  1.006\n",
            "Epoch:  25/100, Batch:    3/4, Training Loss Error:  1.537\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  26/100, Batch:    0/4, Training Loss Error:  0.484\n",
            "Epoch:  26/100, Batch:    1/4, Training Loss Error:  1.080\n",
            "Epoch:  26/100, Batch:    2/4, Training Loss Error:  0.858\n",
            "Epoch:  26/100, Batch:    3/4, Training Loss Error:  1.349\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  27/100, Batch:    0/4, Training Loss Error:  0.448\n",
            "Epoch:  27/100, Batch:    1/4, Training Loss Error:  0.989\n",
            "Epoch:  27/100, Batch:    2/4, Training Loss Error:  0.819\n",
            "Epoch:  27/100, Batch:    3/4, Training Loss Error:  1.325\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  28/100, Batch:    0/4, Training Loss Error:  0.442\n",
            "Epoch:  28/100, Batch:    1/4, Training Loss Error:  0.996\n",
            "Epoch:  28/100, Batch:    2/4, Training Loss Error:  0.845\n",
            "Epoch:  28/100, Batch:    3/4, Training Loss Error:  1.388\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  29/100, Batch:    0/4, Training Loss Error:  0.452\n",
            "Epoch:  29/100, Batch:    1/4, Training Loss Error:  1.049\n",
            "Epoch:  29/100, Batch:    2/4, Training Loss Error:  0.889\n",
            "Epoch:  29/100, Batch:    3/4, Training Loss Error:  1.469\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  30/100, Batch:    0/4, Training Loss Error:  0.462\n",
            "Epoch:  30/100, Batch:    1/4, Training Loss Error:  1.099\n",
            "Epoch:  30/100, Batch:    2/4, Training Loss Error:  0.923\n",
            "Epoch:  30/100, Batch:    3/4, Training Loss Error:  1.505\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  31/100, Batch:    0/4, Training Loss Error:  0.464\n",
            "Epoch:  31/100, Batch:    1/4, Training Loss Error:  1.123\n",
            "Epoch:  31/100, Batch:    2/4, Training Loss Error:  0.943\n",
            "Epoch:  31/100, Batch:    3/4, Training Loss Error:  1.503\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  32/100, Batch:    0/4, Training Loss Error:  0.461\n",
            "Epoch:  32/100, Batch:    1/4, Training Loss Error:  1.135\n",
            "Epoch:  32/100, Batch:    2/4, Training Loss Error:  0.954\n",
            "Epoch:  32/100, Batch:    3/4, Training Loss Error:  1.506\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  33/100, Batch:    0/4, Training Loss Error:  0.459\n",
            "Epoch:  33/100, Batch:    1/4, Training Loss Error:  1.152\n",
            "Epoch:  33/100, Batch:    2/4, Training Loss Error:  0.965\n",
            "Epoch:  33/100, Batch:    3/4, Training Loss Error:  1.525\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  34/100, Batch:    0/4, Training Loss Error:  0.459\n",
            "Epoch:  34/100, Batch:    1/4, Training Loss Error:  1.172\n",
            "Epoch:  34/100, Batch:    2/4, Training Loss Error:  0.981\n",
            "Epoch:  34/100, Batch:    3/4, Training Loss Error:  1.544\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  35/100, Batch:    0/4, Training Loss Error:  0.461\n",
            "Epoch:  35/100, Batch:    1/4, Training Loss Error:  1.195\n",
            "Epoch:  35/100, Batch:    2/4, Training Loss Error:  1.001\n",
            "Epoch:  35/100, Batch:    3/4, Training Loss Error:  1.554\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  36/100, Batch:    0/4, Training Loss Error:  0.462\n",
            "Epoch:  36/100, Batch:    1/4, Training Loss Error:  1.214\n",
            "Epoch:  36/100, Batch:    2/4, Training Loss Error:  1.020\n",
            "Epoch:  36/100, Batch:    3/4, Training Loss Error:  1.559\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  37/100, Batch:    0/4, Training Loss Error:  0.462\n",
            "Epoch:  37/100, Batch:    1/4, Training Loss Error:  1.232\n",
            "Epoch:  37/100, Batch:    2/4, Training Loss Error:  1.032\n",
            "Epoch:  37/100, Batch:    3/4, Training Loss Error:  1.559\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  38/100, Batch:    0/4, Training Loss Error:  0.459\n",
            "Epoch:  38/100, Batch:    1/4, Training Loss Error:  1.248\n",
            "Epoch:  38/100, Batch:    2/4, Training Loss Error:  1.045\n",
            "Epoch:  38/100, Batch:    3/4, Training Loss Error:  1.574\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  39/100, Batch:    0/4, Training Loss Error:  0.462\n",
            "Epoch:  39/100, Batch:    1/4, Training Loss Error:  1.287\n",
            "Epoch:  39/100, Batch:    2/4, Training Loss Error:  1.052\n",
            "Epoch:  39/100, Batch:    3/4, Training Loss Error:  1.566\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  40/100, Batch:    0/4, Training Loss Error:  0.457\n",
            "Epoch:  40/100, Batch:    1/4, Training Loss Error:  1.246\n",
            "Epoch:  40/100, Batch:    2/4, Training Loss Error:  1.058\n",
            "Epoch:  40/100, Batch:    3/4, Training Loss Error:  1.571\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  41/100, Batch:    0/4, Training Loss Error:  0.458\n",
            "Epoch:  41/100, Batch:    1/4, Training Loss Error:  1.251\n",
            "Epoch:  41/100, Batch:    2/4, Training Loss Error:  1.089\n",
            "Epoch:  41/100, Batch:    3/4, Training Loss Error:  1.607\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  42/100, Batch:    0/4, Training Loss Error:  0.468\n",
            "Epoch:  42/100, Batch:    1/4, Training Loss Error:  1.292\n",
            "Epoch:  42/100, Batch:    2/4, Training Loss Error:  1.110\n",
            "Epoch:  42/100, Batch:    3/4, Training Loss Error:  1.621\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  43/100, Batch:    0/4, Training Loss Error:  0.468\n",
            "Epoch:  43/100, Batch:    1/4, Training Loss Error:  1.289\n",
            "Epoch:  43/100, Batch:    2/4, Training Loss Error:  1.099\n",
            "Epoch:  43/100, Batch:    3/4, Training Loss Error:  1.620\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  44/100, Batch:    0/4, Training Loss Error:  0.462\n",
            "Epoch:  44/100, Batch:    1/4, Training Loss Error:  1.282\n",
            "Epoch:  44/100, Batch:    2/4, Training Loss Error:  1.099\n",
            "Epoch:  44/100, Batch:    3/4, Training Loss Error:  1.632\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  45/100, Batch:    0/4, Training Loss Error:  0.466\n",
            "Epoch:  45/100, Batch:    1/4, Training Loss Error:  1.320\n",
            "Epoch:  45/100, Batch:    2/4, Training Loss Error:  1.126\n",
            "Epoch:  45/100, Batch:    3/4, Training Loss Error:  1.647\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  46/100, Batch:    0/4, Training Loss Error:  0.469\n",
            "Epoch:  46/100, Batch:    1/4, Training Loss Error:  1.332\n",
            "Epoch:  46/100, Batch:    2/4, Training Loss Error:  1.132\n",
            "Epoch:  46/100, Batch:    3/4, Training Loss Error:  1.655\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  47/100, Batch:    0/4, Training Loss Error:  0.469\n",
            "Epoch:  47/100, Batch:    1/4, Training Loss Error:  1.330\n",
            "Epoch:  47/100, Batch:    2/4, Training Loss Error:  1.139\n",
            "Epoch:  47/100, Batch:    3/4, Training Loss Error:  1.669\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  48/100, Batch:    0/4, Training Loss Error:  0.476\n",
            "Epoch:  48/100, Batch:    1/4, Training Loss Error:  1.364\n",
            "Epoch:  48/100, Batch:    2/4, Training Loss Error:  1.156\n",
            "Epoch:  48/100, Batch:    3/4, Training Loss Error:  1.682\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  49/100, Batch:    0/4, Training Loss Error:  0.477\n",
            "Epoch:  49/100, Batch:    1/4, Training Loss Error:  1.366\n",
            "Epoch:  49/100, Batch:    2/4, Training Loss Error:  1.155\n",
            "Epoch:  49/100, Batch:    3/4, Training Loss Error:  1.688\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  50/100, Batch:    0/4, Training Loss Error:  0.478\n",
            "Epoch:  50/100, Batch:    1/4, Training Loss Error:  1.368\n",
            "Epoch:  50/100, Batch:    2/4, Training Loss Error:  1.165\n",
            "Epoch:  50/100, Batch:    3/4, Training Loss Error:  1.696\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  51/100, Batch:    0/4, Training Loss Error:  0.486\n",
            "Epoch:  51/100, Batch:    1/4, Training Loss Error:  1.400\n",
            "Epoch:  51/100, Batch:    2/4, Training Loss Error:  1.177\n",
            "Epoch:  51/100, Batch:    3/4, Training Loss Error:  1.703\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  52/100, Batch:    0/4, Training Loss Error:  0.486\n",
            "Epoch:  52/100, Batch:    1/4, Training Loss Error:  1.393\n",
            "Epoch:  52/100, Batch:    2/4, Training Loss Error:  1.173\n",
            "Epoch:  52/100, Batch:    3/4, Training Loss Error:  1.714\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  53/100, Batch:    0/4, Training Loss Error:  0.490\n",
            "Epoch:  53/100, Batch:    1/4, Training Loss Error:  1.416\n",
            "Epoch:  53/100, Batch:    2/4, Training Loss Error:  1.187\n",
            "Epoch:  53/100, Batch:    3/4, Training Loss Error:  1.726\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  54/100, Batch:    0/4, Training Loss Error:  0.494\n",
            "Epoch:  54/100, Batch:    1/4, Training Loss Error:  1.419\n",
            "Epoch:  54/100, Batch:    2/4, Training Loss Error:  1.187\n",
            "Epoch:  54/100, Batch:    3/4, Training Loss Error:  1.731\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  55/100, Batch:    0/4, Training Loss Error:  0.498\n",
            "Epoch:  55/100, Batch:    1/4, Training Loss Error:  1.430\n",
            "Epoch:  55/100, Batch:    2/4, Training Loss Error:  1.194\n",
            "Epoch:  55/100, Batch:    3/4, Training Loss Error:  1.739\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  56/100, Batch:    0/4, Training Loss Error:  0.504\n",
            "Epoch:  56/100, Batch:    1/4, Training Loss Error:  1.444\n",
            "Epoch:  56/100, Batch:    2/4, Training Loss Error:  1.198\n",
            "Epoch:  56/100, Batch:    3/4, Training Loss Error:  1.748\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  57/100, Batch:    0/4, Training Loss Error:  0.506\n",
            "Epoch:  57/100, Batch:    1/4, Training Loss Error:  1.445\n",
            "Epoch:  57/100, Batch:    2/4, Training Loss Error:  1.204\n",
            "Epoch:  57/100, Batch:    3/4, Training Loss Error:  1.755\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  58/100, Batch:    0/4, Training Loss Error:  0.514\n",
            "Epoch:  58/100, Batch:    1/4, Training Loss Error:  1.463\n",
            "Epoch:  58/100, Batch:    2/4, Training Loss Error:  1.204\n",
            "Epoch:  58/100, Batch:    3/4, Training Loss Error:  1.760\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  59/100, Batch:    0/4, Training Loss Error:  0.513\n",
            "Epoch:  59/100, Batch:    1/4, Training Loss Error:  1.456\n",
            "Epoch:  59/100, Batch:    2/4, Training Loss Error:  1.214\n",
            "Epoch:  59/100, Batch:    3/4, Training Loss Error:  1.769\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  60/100, Batch:    0/4, Training Loss Error:  0.526\n",
            "Epoch:  60/100, Batch:    1/4, Training Loss Error:  1.481\n",
            "Epoch:  60/100, Batch:    2/4, Training Loss Error:  1.209\n",
            "Epoch:  60/100, Batch:    3/4, Training Loss Error:  1.769\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  61/100, Batch:    0/4, Training Loss Error:  0.518\n",
            "Epoch:  61/100, Batch:    1/4, Training Loss Error:  1.466\n",
            "Epoch:  61/100, Batch:    2/4, Training Loss Error:  1.220\n",
            "Epoch:  61/100, Batch:    3/4, Training Loss Error:  1.779\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  62/100, Batch:    0/4, Training Loss Error:  0.535\n",
            "Epoch:  62/100, Batch:    1/4, Training Loss Error:  1.501\n",
            "Epoch:  62/100, Batch:    2/4, Training Loss Error:  1.213\n",
            "Epoch:  62/100, Batch:    3/4, Training Loss Error:  1.780\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  63/100, Batch:    0/4, Training Loss Error:  0.518\n",
            "Epoch:  63/100, Batch:    1/4, Training Loss Error:  1.481\n",
            "Epoch:  63/100, Batch:    2/4, Training Loss Error:  1.227\n",
            "Epoch:  63/100, Batch:    3/4, Training Loss Error:  1.801\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  64/100, Batch:    0/4, Training Loss Error:  0.561\n",
            "Epoch:  64/100, Batch:    1/4, Training Loss Error:  1.538\n",
            "Epoch:  64/100, Batch:    2/4, Training Loss Error:  1.199\n",
            "Epoch:  64/100, Batch:    3/4, Training Loss Error:  1.774\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  65/100, Batch:    0/4, Training Loss Error:  0.502\n",
            "Epoch:  65/100, Batch:    1/4, Training Loss Error:  1.488\n",
            "Epoch:  65/100, Batch:    2/4, Training Loss Error:  1.238\n",
            "Epoch:  65/100, Batch:    3/4, Training Loss Error:  1.841\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  66/100, Batch:    0/4, Training Loss Error:  0.564\n",
            "Epoch:  66/100, Batch:    1/4, Training Loss Error:  1.578\n",
            "Epoch:  66/100, Batch:    2/4, Training Loss Error:  1.241\n",
            "Epoch:  66/100, Batch:    3/4, Training Loss Error:  1.826\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  67/100, Batch:    0/4, Training Loss Error:  0.533\n",
            "Epoch:  67/100, Batch:    1/4, Training Loss Error:  1.476\n",
            "Epoch:  67/100, Batch:    2/4, Training Loss Error:  1.216\n",
            "Epoch:  67/100, Batch:    3/4, Training Loss Error:  1.855\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  68/100, Batch:    0/4, Training Loss Error:  0.548\n",
            "Epoch:  68/100, Batch:    1/4, Training Loss Error:  1.519\n",
            "Epoch:  68/100, Batch:    2/4, Training Loss Error:  1.232\n",
            "Epoch:  68/100, Batch:    3/4, Training Loss Error:  1.913\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  69/100, Batch:    0/4, Training Loss Error:  0.545\n",
            "Epoch:  69/100, Batch:    1/4, Training Loss Error:  1.568\n",
            "Epoch:  69/100, Batch:    2/4, Training Loss Error:  1.305\n",
            "Epoch:  69/100, Batch:    3/4, Training Loss Error:  1.915\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  70/100, Batch:    0/4, Training Loss Error:  0.527\n",
            "Epoch:  70/100, Batch:    1/4, Training Loss Error:  1.487\n",
            "Epoch:  70/100, Batch:    2/4, Training Loss Error:  1.257\n",
            "Epoch:  70/100, Batch:    3/4, Training Loss Error:  1.873\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  71/100, Batch:    0/4, Training Loss Error:  0.551\n",
            "Epoch:  71/100, Batch:    1/4, Training Loss Error:  1.536\n",
            "Epoch:  71/100, Batch:    2/4, Training Loss Error:  1.358\n",
            "Epoch:  71/100, Batch:    3/4, Training Loss Error:  2.103\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  72/100, Batch:    0/4, Training Loss Error:  0.548\n",
            "Epoch:  72/100, Batch:    1/4, Training Loss Error:  1.493\n",
            "Epoch:  72/100, Batch:    2/4, Training Loss Error:  1.393\n",
            "Epoch:  72/100, Batch:    3/4, Training Loss Error:  2.076\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  73/100, Batch:    0/4, Training Loss Error:  0.615\n",
            "Epoch:  73/100, Batch:    1/4, Training Loss Error:  1.682\n",
            "Epoch:  73/100, Batch:    2/4, Training Loss Error:  1.383\n",
            "Epoch:  73/100, Batch:    3/4, Training Loss Error:  2.027\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  74/100, Batch:    0/4, Training Loss Error:  0.620\n",
            "Epoch:  74/100, Batch:    1/4, Training Loss Error:  1.594\n",
            "Epoch:  74/100, Batch:    2/4, Training Loss Error:  1.313\n",
            "Epoch:  74/100, Batch:    3/4, Training Loss Error:  1.902\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  75/100, Batch:    0/4, Training Loss Error:  0.603\n",
            "Epoch:  75/100, Batch:    1/4, Training Loss Error:  1.566\n",
            "Epoch:  75/100, Batch:    2/4, Training Loss Error:  1.327\n",
            "Epoch:  75/100, Batch:    3/4, Training Loss Error:  1.853\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  76/100, Batch:    0/4, Training Loss Error:  0.590\n",
            "Epoch:  76/100, Batch:    1/4, Training Loss Error:  1.545\n",
            "Epoch:  76/100, Batch:    2/4, Training Loss Error:  1.316\n",
            "Epoch:  76/100, Batch:    3/4, Training Loss Error:  1.884\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  77/100, Batch:    0/4, Training Loss Error:  0.586\n",
            "Epoch:  77/100, Batch:    1/4, Training Loss Error:  1.558\n",
            "Epoch:  77/100, Batch:    2/4, Training Loss Error:  1.321\n",
            "Epoch:  77/100, Batch:    3/4, Training Loss Error:  1.942\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  78/100, Batch:    0/4, Training Loss Error:  0.588\n",
            "Epoch:  78/100, Batch:    1/4, Training Loss Error:  1.593\n",
            "Epoch:  78/100, Batch:    2/4, Training Loss Error:  1.332\n",
            "Epoch:  78/100, Batch:    3/4, Training Loss Error:  1.992\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  79/100, Batch:    0/4, Training Loss Error:  0.588\n",
            "Epoch:  79/100, Batch:    1/4, Training Loss Error:  1.613\n",
            "Epoch:  79/100, Batch:    2/4, Training Loss Error:  1.335\n",
            "Epoch:  79/100, Batch:    3/4, Training Loss Error:  2.011\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  80/100, Batch:    0/4, Training Loss Error:  0.584\n",
            "Epoch:  80/100, Batch:    1/4, Training Loss Error:  1.612\n",
            "Epoch:  80/100, Batch:    2/4, Training Loss Error:  1.324\n",
            "Epoch:  80/100, Batch:    3/4, Training Loss Error:  1.994\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  81/100, Batch:    0/4, Training Loss Error:  0.579\n",
            "Epoch:  81/100, Batch:    1/4, Training Loss Error:  1.593\n",
            "Epoch:  81/100, Batch:    2/4, Training Loss Error:  1.305\n",
            "Epoch:  81/100, Batch:    3/4, Training Loss Error:  1.959\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  82/100, Batch:    0/4, Training Loss Error:  0.573\n",
            "Epoch:  82/100, Batch:    1/4, Training Loss Error:  1.570\n",
            "Epoch:  82/100, Batch:    2/4, Training Loss Error:  1.284\n",
            "Epoch:  82/100, Batch:    3/4, Training Loss Error:  1.924\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  83/100, Batch:    0/4, Training Loss Error:  0.569\n",
            "Epoch:  83/100, Batch:    1/4, Training Loss Error:  1.553\n",
            "Epoch:  83/100, Batch:    2/4, Training Loss Error:  1.269\n",
            "Epoch:  83/100, Batch:    3/4, Training Loss Error:  1.902\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  84/100, Batch:    0/4, Training Loss Error:  0.568\n",
            "Epoch:  84/100, Batch:    1/4, Training Loss Error:  1.545\n",
            "Epoch:  84/100, Batch:    2/4, Training Loss Error:  1.261\n",
            "Epoch:  84/100, Batch:    3/4, Training Loss Error:  1.893\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  85/100, Batch:    0/4, Training Loss Error:  0.568\n",
            "Epoch:  85/100, Batch:    1/4, Training Loss Error:  1.540\n",
            "Epoch:  85/100, Batch:    2/4, Training Loss Error:  1.260\n",
            "Epoch:  85/100, Batch:    3/4, Training Loss Error:  1.892\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  86/100, Batch:    0/4, Training Loss Error:  0.569\n",
            "Epoch:  86/100, Batch:    1/4, Training Loss Error:  1.537\n",
            "Epoch:  86/100, Batch:    2/4, Training Loss Error:  1.262\n",
            "Epoch:  86/100, Batch:    3/4, Training Loss Error:  1.893\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  87/100, Batch:    0/4, Training Loss Error:  0.569\n",
            "Epoch:  87/100, Batch:    1/4, Training Loss Error:  1.534\n",
            "Epoch:  87/100, Batch:    2/4, Training Loss Error:  1.263\n",
            "Epoch:  87/100, Batch:    3/4, Training Loss Error:  1.893\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  88/100, Batch:    0/4, Training Loss Error:  0.569\n",
            "Epoch:  88/100, Batch:    1/4, Training Loss Error:  1.532\n",
            "Epoch:  88/100, Batch:    2/4, Training Loss Error:  1.264\n",
            "Epoch:  88/100, Batch:    3/4, Training Loss Error:  1.893\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  89/100, Batch:    0/4, Training Loss Error:  0.569\n",
            "Epoch:  89/100, Batch:    1/4, Training Loss Error:  1.534\n",
            "Epoch:  89/100, Batch:    2/4, Training Loss Error:  1.265\n",
            "Epoch:  89/100, Batch:    3/4, Training Loss Error:  1.893\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  90/100, Batch:    0/4, Training Loss Error:  0.570\n",
            "Epoch:  90/100, Batch:    1/4, Training Loss Error:  1.537\n",
            "Epoch:  90/100, Batch:    2/4, Training Loss Error:  1.266\n",
            "Epoch:  90/100, Batch:    3/4, Training Loss Error:  1.891\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  91/100, Batch:    0/4, Training Loss Error:  0.571\n",
            "Epoch:  91/100, Batch:    1/4, Training Loss Error:  1.541\n",
            "Epoch:  91/100, Batch:    2/4, Training Loss Error:  1.269\n",
            "Epoch:  91/100, Batch:    3/4, Training Loss Error:  1.889\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  92/100, Batch:    0/4, Training Loss Error:  0.573\n",
            "Epoch:  92/100, Batch:    1/4, Training Loss Error:  1.544\n",
            "Epoch:  92/100, Batch:    2/4, Training Loss Error:  1.272\n",
            "Epoch:  92/100, Batch:    3/4, Training Loss Error:  1.887\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  93/100, Batch:    0/4, Training Loss Error:  0.575\n",
            "Epoch:  93/100, Batch:    1/4, Training Loss Error:  1.546\n",
            "Epoch:  93/100, Batch:    2/4, Training Loss Error:  1.275\n",
            "Epoch:  93/100, Batch:    3/4, Training Loss Error:  1.885\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  94/100, Batch:    0/4, Training Loss Error:  0.578\n",
            "Epoch:  94/100, Batch:    1/4, Training Loss Error:  1.549\n",
            "Epoch:  94/100, Batch:    2/4, Training Loss Error:  1.278\n",
            "Epoch:  94/100, Batch:    3/4, Training Loss Error:  1.886\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  95/100, Batch:    0/4, Training Loss Error:  0.581\n",
            "Epoch:  95/100, Batch:    1/4, Training Loss Error:  1.554\n",
            "Epoch:  95/100, Batch:    2/4, Training Loss Error:  1.281\n",
            "Epoch:  95/100, Batch:    3/4, Training Loss Error:  1.888\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  96/100, Batch:    0/4, Training Loss Error:  0.584\n",
            "Epoch:  96/100, Batch:    1/4, Training Loss Error:  1.560\n",
            "Epoch:  96/100, Batch:    2/4, Training Loss Error:  1.284\n",
            "Epoch:  96/100, Batch:    3/4, Training Loss Error:  1.889\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  97/100, Batch:    0/4, Training Loss Error:  0.586\n",
            "Epoch:  97/100, Batch:    1/4, Training Loss Error:  1.564\n",
            "Epoch:  97/100, Batch:    2/4, Training Loss Error:  1.287\n",
            "Epoch:  97/100, Batch:    3/4, Training Loss Error:  1.889\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  98/100, Batch:    0/4, Training Loss Error:  0.588\n",
            "Epoch:  98/100, Batch:    1/4, Training Loss Error:  1.567\n",
            "Epoch:  98/100, Batch:    2/4, Training Loss Error:  1.289\n",
            "Epoch:  98/100, Batch:    3/4, Training Loss Error:  1.889\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  99/100, Batch:    0/4, Training Loss Error:  0.590\n",
            "Epoch:  99/100, Batch:    1/4, Training Loss Error:  1.571\n",
            "Epoch:  99/100, Batch:    2/4, Training Loss Error:  1.292\n",
            "Epoch:  99/100, Batch:    3/4, Training Loss Error:  1.890\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch: 100/100, Batch:    0/4, Training Loss Error:  0.593\n",
            "Epoch: 100/100, Batch:    1/4, Training Loss Error:  1.574\n",
            "Epoch: 100/100, Batch:    2/4, Training Loss Error:  1.294\n",
            "Epoch: 100/100, Batch:    3/4, Training Loss Error:  1.890\n",
            "No validation batches were processed.\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Game Over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_model.save_weights(\"./chatbot_weights.ckpt\")\n"
      ],
      "metadata": {
        "id": "s50dl0-r2yzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KW43bsPk2y2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1Lq6O6W2y46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING THE SEQ2SEQ MODEL"
      ],
      "metadata": {
        "id": "QgOUYdNA44WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Loading the weights\n",
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "seq2seq_model.load_weights(checkpoint)\n",
        "\n",
        "def convert_string2int(question, word2int):\n",
        "    question = clean_text(question)\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]\n",
        "\n",
        "sequence_length = 25\n",
        "batch_size = 32\n",
        "\n",
        "def chat_with_model(input_text):\n",
        "    input_ints = convert_string2int(input_text, questionswords2int)\n",
        "    input_padded = input_ints + [questionswords2int['<PAD>']] * (sequence_length - len(input_ints))\n",
        "    \n",
        "    fake_batch = np.zeros((batch_size, sequence_length))\n",
        "    fake_batch[0] = input_padded\n",
        "    \n",
        "    logits = seq2seq_model.predict([fake_batch, np.zeros((batch_size, sequence_length - 1))])\n",
        "    response_indices = np.argmax(logits, axis=-1)[0]\n",
        "    \n",
        "    response_tokens = []\n",
        "    for index in response_indices:\n",
        "        if answersints2word[index] == 'i':\n",
        "            token = ' I'\n",
        "        elif answersints2word[index] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif answersints2word[index] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + answersints2word[index]\n",
        "        response_tokens.append(token)\n",
        "        if token == '.':\n",
        "            break\n",
        "    \n",
        "    response_text = ''.join(response_tokens)\n",
        "    return response_text\n",
        "\n",
        "# Setting up the chat\n",
        "while True:\n",
        "    question = input(\"You: \")\n",
        "    if question.lower() == 'goodbye':\n",
        "        break\n",
        "    response = chat_with_model(question)\n",
        "    print('ChatBot:', response)\n"
      ],
      "metadata": {
        "id": "Wm44kdw92y7Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "8ae4cde8-3129-4997-8611-06751abe3176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: What does this section show different approaches to working with chained maps?\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "ChatBot: .\n",
            "You: What class can be used to simulate nested contexts?\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "ChatBot: .\n",
            "You: What does the ChainMap class only make updates to?\n",
            "1/1 [==============================] - 1s 780ms/step\n",
            "ChatBot: .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-d376bff85aaa>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Setting up the chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'goodbye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3dmQ0jl1_dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9lIUIm11_fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}